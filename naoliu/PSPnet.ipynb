{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.core import Activation, Dropout, Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    return (2. * K.sum(y_true * y_pred) + 1.) / (K.sum(y_true) + K.sum(y_pred) + 1.)\n",
    "\n",
    "\n",
    "def conv_block(input_tensor, filters, strides, d_rates):\n",
    "    x = Conv2D(filters[0], kernel_size=1, dilation_rate=d_rates[0])(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters[1], kernel_size=3, strides=strides, padding='same', dilation_rate=d_rates[1])(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters[2], kernel_size=1, dilation_rate=d_rates[2])(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    shortcut = Conv2D(filters[2], kernel_size=1, strides=strides)(input_tensor)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def identity_block(input_tensor, filters, d_rates):\n",
    "    x = Conv2D(filters[0], kernel_size=1, dilation_rate=d_rates[0])(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters[1], kernel_size=3, padding='same', dilation_rate=d_rates[1])(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters[2], kernel_size=1, dilation_rate=d_rates[2])(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = add([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def pyramid_pooling_block(input_tensor, bin_sizes):\n",
    "    concat_list = [input_tensor]\n",
    "    h = input_tensor.shape[1].value\n",
    "    w = input_tensor.shape[2].value\n",
    "\n",
    "    for bin_size in bin_sizes:\n",
    "        x = AveragePooling2D(pool_size=(h//bin_size, w//bin_size), strides=(h//bin_size, w//bin_size))(input_tensor)\n",
    "        x = Conv2D(512, kernel_size=1)(x)\n",
    "        x = Lambda(lambda x: tf.image.resize_images(x, (h, w)))(x)\n",
    "\n",
    "        concat_list.append(x)\n",
    "\n",
    "    return concatenate(concat_list)\n",
    "\n",
    "\n",
    "def pspnet50(num_classes, input_shape, lr_init, lr_decay):\n",
    "    img_input = Input(input_shape)\n",
    "\n",
    "    x = Conv2D(64, kernel_size=3, strides=(2, 2), padding='same')(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(64, kernel_size=3, strides=(1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(128, kernel_size=3, strides=(1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "\n",
    "    x = conv_block(x, filters=[64, 64, 256], strides=(1, 1), d_rates=[1, 1, 1])\n",
    "    x = identity_block(x, filters=[64, 64, 256], d_rates=[1, 1, 1])\n",
    "    x = identity_block(x, filters=[64, 64, 256], d_rates=[1, 1, 1])\n",
    "\n",
    "    x = conv_block(x, filters=[128, 128, 512], strides=(2, 2), d_rates=[1, 1, 1])\n",
    "    x = identity_block(x, filters=[128, 128, 512], d_rates=[1, 1, 1])\n",
    "    x = identity_block(x, filters=[128, 128, 512], d_rates=[1, 1, 1])\n",
    "    x = identity_block(x, filters=[128, 128, 512], d_rates=[1, 1, 1])\n",
    "\n",
    "    x = conv_block(x, filters=[256, 256, 1024], strides=(1, 1), d_rates=[1, 2, 1])\n",
    "    x = identity_block(x, filters=[256, 256, 1024], d_rates=[1, 2, 1])\n",
    "    x = identity_block(x, filters=[256, 256, 1024], d_rates=[1, 2, 1])\n",
    "    x = identity_block(x, filters=[256, 256, 1024], d_rates=[1, 2, 1])\n",
    "    x = identity_block(x, filters=[256, 256, 1024], d_rates=[1, 2, 1])\n",
    "    x = identity_block(x, filters=[256, 256, 1024], d_rates=[1, 2, 1])\n",
    "\n",
    "    x = conv_block(x, filters=[512, 512, 2048], strides=(1, 1), d_rates=[1, 4, 1])\n",
    "    x = identity_block(x, filters=[512, 512, 2048], d_rates=[1, 4, 1])\n",
    "    x = identity_block(x, filters=[512, 512, 2048], d_rates=[1, 4, 1])\n",
    "\n",
    "    x = pyramid_pooling_block(x, [1, 2, 3, 6])\n",
    "\n",
    "    x = Conv2D(512, kernel_size=3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    x = Conv2D(num_classes, kernel_size=1)(x)\n",
    "    x = Conv2DTranspose(num_classes, kernel_size=(16, 16), strides=(8, 8), padding='same')(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "\n",
    "    model = Model(img_input, x)\n",
    "    model.compile(optimizer=Adam(lr=lr_init, decay=lr_decay),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc',dice_coef])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(output=\"./logs\"):\n",
    "    print('start')\n",
    "    x_train = np.load('./x_train_truck_640.npy')\n",
    "    y_train = np.load('./y_train_truck_640.npy')\n",
    "    #y_train = y_train.reshape((600, 640, 360, 1))\n",
    "    y_train = y_train.reshape((450, 640, 360, 1))\n",
    "    x_test = np.load('./x_test_truck_640.npy')\n",
    "    y_test = np.load('./y_test_truck_640.npy')\n",
    "    y_test = y_test.reshape((164, 640, 360, 1))\n",
    "    #y_test = y_test.reshape((216, 640, 360, 1))\n",
    "    #y_train = to_categorical(y_train, 3)\n",
    "    #y_test = to_categorical(y_test, 3)\n",
    "\n",
    "    print(x_train.shape, y_train.shape, y_test.shape)\n",
    "    #x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=8)\n",
    "    score = pspnet50(num_classes=1, input_shape=(640, 360, 3), lr_init=1e-4, lr_decay=5e-4 )\n",
    "    #score = model.load_weights( \"./model/pspnet_100_truck_bc.model\")\n",
    "    #score.compile(optimizer=\"sgd\", loss= dice_coef, metrics=['acc'])\n",
    "    filepath = \"./model/pspnet_100_truck_bc.model\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    score.fit(x_train, y_train, epochs=100, batch_size=4, class_weight = 'auto',validation_data=(x_test, y_test), shuffle=True, callbacks=callbacks_list, verbose=1)\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    print('start')\n",
    "    x_test = np.load('./x_test_container_640.npy')\n",
    "    model = pspnet50(num_classes=3, input_shape=(640, 360, 3), lr_init=1e-4, lr_decay=5e-4 )\n",
    "    score = model.load_weights( \"./model/pspnet_100_container_cc.model\")\n",
    "    print('load')\n",
    "    pred = model.predict(x_test)\n",
    "    np.save('./y_predict_container_psp_cc.npy', pred)\n",
    "    print(pred.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load\n",
      "start\n",
      "load\n",
      "(216, 640, 360, 3)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "     print('load')\n",
    "     #train()\n",
    "     predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
